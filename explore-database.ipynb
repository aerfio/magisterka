{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, minmax_scale\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, random, os, time\n",
    "from collections import deque\n",
    "from IPython.core.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization, GRU\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "SEQ_LEN=48\n",
    "DO_PLOT=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = \"/Users/i354746/private/magisterka/datasets/eurusd-m15-2018/EURUSD.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset_location)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    'Open': 'open', 'Close': 'close',\n",
    "    'High': 'high', 'Low': 'low',\n",
    "    'Close': 'close', 'Volume': 'volume',\n",
    "    \"Date\": \"date\", 'Timestamp': 'timestamp', }, inplace=True)\n",
    "\n",
    "df.drop(\"volume\", 1, inplace=True) # we drop volume as it is not helpful TODO look at it later\n",
    "\n",
    "\n",
    "df[\"timestamp\"] = df[\"date\"].astype(str) + \" \" + df[\"timestamp\"]\n",
    "df.drop(\"date\", 1, inplace=True)\n",
    "df.rename(columns={'Time': 'timestamp', 'Open': 'open', 'Close': 'close',\n",
    "                   'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}, inplace=True)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], infer_datetime_format=True)\n",
    "df.fillna(method=\"ffill\", inplace=True)\n",
    "# df.plot.line(x=\"timestamp\", y=\"close\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], infer_datetime_format=True).astype(int)\n",
    "df[\"timestamp\"] = df[\"timestamp\"]/1000000000\n",
    "df.set_index('timestamp', inplace=True)\n",
    "df = df.astype(float)\n",
    "\n",
    "# Add additional features\n",
    "# df['momentum'] = df['volume'] * (df['open'] - df['close'])\n",
    "# df['avg_price'] = (df['low'] + df['high']) / 2\n",
    "# df['range'] = df['high'] - df['low']\n",
    "# df['ohlc_price'] = (df['low'] + df['high'] + df['open'] + df['close']) / 4\n",
    "# df['oc_diff'] = df['open'] - df['close']\n",
    "\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_PLOT:\n",
    "    df.copy().plot(subplots=True, layout=(3, 4), figsize=(40, 20), sharex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPercentageChange(df):\n",
    "    copied=df.copy()\n",
    "    for col in copied.columns:\n",
    "        if col != \"target\": \n",
    "            copied[col] = copied[col].pct_change()\n",
    "    copied.plot(subplots=True, layout=(3, 4), figsize=(40, 20), sharex=False)      \n",
    "\n",
    "if DO_PLOT:\n",
    "    plotPercentageChange(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift=4 # 1 hour shift\n",
    "df[\"future\"]=df[\"close\"].shift(-shift)\n",
    "df.dropna(inplace=True)\n",
    "df[\"return\"]= df[\"close\"]-df[\"future\"]\n",
    "display(df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"future\", 1, inplace=True)\n",
    "# df[\"target\"]= df[\"return\"].shift(shift)\n",
    "df['target'] = df['return'].apply(lambda x: 1 if x>0.0 else 0)\n",
    "\n",
    "display(df.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times=sorted(df.index.values)\n",
    "last_10pct=times[-int(0.1*len(times))]\n",
    "print(last_10pct, datetime.datetime.fromtimestamp(last_10pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df=df[(df.index >= last_10pct)]\n",
    "df=df[(df.index < last_10pct)]\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df_in):\n",
    "    df=df_in.copy()\n",
    "    df.dropna(inplace=True)\n",
    "    for col in df.columns:\n",
    "        if col != \"target\" and col != \"return\": \n",
    "            # df[col] = df[col].pct_change()\n",
    "            # return\n",
    "            df[col]=minmax_scale(df[col])\n",
    "            # print(col, np.amax(df[col]), np.amin(df[col]))\n",
    "            \n",
    "    df.dropna(inplace=True)  # cleanup again... jic.\n",
    "\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "\n",
    "    for i in df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
    "\n",
    "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "\n",
    "    buys = []  # list that will store our buy sequences and targets\n",
    "    sells = []  # list that will store our sell sequences and targets\n",
    "\n",
    "    for seq, target in sequential_data:  # iterate over the sequential data\n",
    "        if target == 0:  # if it's a \"not buy\"\n",
    "            sells.append([seq, target])  # append to sells list\n",
    "        elif target == 1:  # otherwise if the target is a 1...\n",
    "            buys.append([seq, target])  # it's a buy!\n",
    "\n",
    "    random.shuffle(buys)  # shuffle the buys\n",
    "    random.shuffle(sells)  # shuffle the sells!\n",
    "\n",
    "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
    "\n",
    "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
    "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
    "\n",
    "    sequential_data = buys+sells  # add them together\n",
    "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:  # going over our new sequential data\n",
    "        X.append(seq)  # X is the sequences\n",
    "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
    "\n",
    "    return np.array(X), np.array(y)  # return X and y...and make X a numpy array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "train_x, train_y = preprocess_df(df)\n",
    "validation_x, validation_y = preprocess_df(validation_df)\n",
    "# print(train_x[0])\n",
    "# print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "# print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "# print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_x[0][47])\n",
    "EPOCHS = 10  # how many passes through our data\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{shift}-PRED-{int(time.time())}\"  # a unique name for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
    "\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "# filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "# checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath), monitor='val_acc', verbose=1, save_best_only=True, mode='max') # saves only the best ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model\n",
    "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# Save model\n",
    "model.save(\"models/{}\".format(NAME))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitvenvvenv07056f12f87c4c6fa7d060979708382c",
   "display_name": "Python 3.7.7 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}